{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1\n",
    "Due by 8th May, 2024 at 23:59 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "We want to create a 2 layer NN, which means we want to calculate  $y = W_2 * ReLU(W_1 * x + b_1) + b_2$\n",
    "\n",
    "Complete the TODOs below to create such a NN.\n",
    "\n",
    "Since you will be needing to compute the gradients w.r.t. all parameters, you may look into online resources for help. Please cite or link any online recources you do use.\n",
    "\n",
    "You are allowed to change any existing parts, however the code has to remain easy to understand and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "        \n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise ReLU application\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    # TODO: Write a function given a numpy array that calculates the gradient of the ReLU function w.r.t. `x`\n",
    "    # TODO: Also write the derivation of the gradient in the PDF file In the implementation you may simply use the final derivation.\n",
    "    # Hint: The function should return a numpy array of the same dimension that `x` has, but only containing 0 or 1\n",
    "    arr = np.zeros(x.shape)\n",
    "    return np.greater(x, arr).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyNeuralNet:\n",
    "    \n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        # TODO: Randomly initialize the weight matrices W_1, W_2 and biases b_1, b_2\n",
    "        # Hint: use np.random.randn() and make sure to correctly set the dimensions \n",
    "\n",
    "        # Scale random sample with 0.01 according to lecture\n",
    "        self.W_1 = 0.01 * np.random.randn(self.dim_in, self.dim_hidden)\n",
    "        self.b_1 = 0.01 * np.random.randn(self.dim_hidden)\n",
    "        self.W_2 = 0.01 * np.random.randn(self.dim_hidden, self.dim_out)\n",
    "        self.b_2 = 0.01 * np.random.randn(self.dim_out)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of the neural network for the given x.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): predicted output for `x`\n",
    "        \"\"\"\n",
    "        # TODO: Calculate output self.out\n",
    "        # Safe intermediate results as cache for later backpropagation\n",
    "        self.h_1 = np.dot(x, self.W_1) + self.b_1\n",
    "        self.h_1_act = relu(self.h_1)\n",
    "        self.out = np.dot(self.h_1_act, self.W_2) + self.b_2\n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the Mean-Squared Error and returns the gradients w.r.t. to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array with shape (self.dim_in, n)\n",
    "            y (np.ndarray): ground truth value numpy array with shape (self.dim_out, n)\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Mean-Squared-Error between predicted value on input points and ground truth value\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1   \n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2  \n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1   \n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2   \n",
    "        \"\"\"\n",
    "        # TODO: Calculate the loss (Mean-Squared-Error)\n",
    "        # Hint: use np.square() and np.mean()\n",
    "\n",
    "        y_pred = self.predict(x)\n",
    "        loss = np.mean(np.square(y_pred - y))\n",
    "        \n",
    "        # TODO: Calculate all gradients w.r.t to the parameters\n",
    "        # Hint: You need to calculate the gradients for each of the parameters by hand\n",
    "        # TODO: Also write the derivation of the gradient in the PDF file. In the implementation you may simply use the final derivation.\n",
    "\n",
    "        loss_derived = 2 * (self.out - y) / len(y)\n",
    "        \n",
    "        h_1_grad = np.dot(loss_derived, self.W_2.T) * relu_grad(self.h_1)\n",
    "        \n",
    "        W_2_grad = np.dot(self.h_1_act.T, loss_derived)\n",
    "        b_2_grad = np.sum(loss_derived, axis=0)\n",
    "        W_1_grad = np.dot(x.T, h_1_grad)\n",
    "        b_1_grad = np.sum(h_1_grad, axis=0)\n",
    "\n",
    "        return loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad\n",
    "         \n",
    "    def train(self, x, y, lr=0.001, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input values\n",
    "            y (np.ndarray): ground truth values\n",
    "            lr (float): learning rate, default: 0.001\n",
    "            epochs (int): number of epochs to train, default: 1000\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Return the loss achieved after all epochs\n",
    "        \"\"\"\n",
    "        # TODO: Keep track of the loss\n",
    "        loss_history = []\n",
    "        n = len(x)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # shuffle dataset\n",
    "            idx = np.arange(n)\n",
    "            np.random.shuffle(idx)\n",
    "            x_shuffled = x[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            for i in range(n):\n",
    "                dp_x = x_shuffled[i]\n",
    "                dp_y = y_shuffled[i]\n",
    "                \n",
    "                loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad = self.loss(dp_x.reshape(1,-1), dp_y.reshape(1,-1))\n",
    "                \n",
    "                self.W_1 -= lr * W_1_grad\n",
    "                self.W_2 -= lr * W_2_grad\n",
    "                self.b_1 -= lr * b_1_grad\n",
    "                self.b_2 -= lr * b_2_grad\n",
    "\n",
    "                loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            # print mean loss of dataset every 10% of epochs\n",
    "            e = int(epochs / 10)\n",
    "            if epoch % e == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {np.mean(loss_history[-n:])}')\n",
    "\n",
    "        \n",
    "        # TODO: Plot the loss history and return the loss achieved after the final epoch\n",
    "        # Plot the loss history after every epoch. Returned is the mean loss of the last epoch.\n",
    "        loss_per_epoch = [np.mean(loss_history[i*n:(i+1)*n]) for i in range(epochs)]\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.show()\n",
    "\n",
    "        return loss_per_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3666745753743754\n",
      "Epoch 100, Loss: 0.0036042473205988306\n",
      "Epoch 200, Loss: 0.0013828907632766091\n",
      "Epoch 300, Loss: 0.0008922379773640516\n",
      "Epoch 400, Loss: 0.0007103713863246147\n",
      "Epoch 500, Loss: 0.0006294838922891847\n",
      "Epoch 600, Loss: 0.0005601103028571541\n",
      "Epoch 700, Loss: 0.0005189778265737468\n",
      "Epoch 800, Loss: 0.0004907267323295599\n",
      "Epoch 900, Loss: 0.00046427101441351735\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after final epoch: 0.0004366728639815288\n",
      "Example predictions of model: \n",
      "x:0.4 y:[[0.17792394]]\n",
      "x:0.5 y:[[0.25826863]]\n",
      "x:0.7 y:[[0.4666888]]\n",
      "\n",
      " if these examples are predicted badly, the model might only perform well on training set\n"
     ]
    }
   ],
   "source": [
    "# We test the model created above on the simple function y = x^2\n",
    "\n",
    "model = NumPyNeuralNet(1, 30, 1)\n",
    "\n",
    "# Create a randomly distributed array of 1000 values between 0 and 1\n",
    "x_train = 1 * np.random.randn(1000, 1)\n",
    "# Create ground truth by calculating x*x\n",
    "y_train = x_train * x_train\n",
    "\n",
    "# Train for default epochs\n",
    "loss = model.train(x_train, y_train)\n",
    "print(\"loss after final epoch: \" + str(loss))\n",
    "example_1 = 0.4\n",
    "example_2 = 0.5\n",
    "example_3 = 0.7\n",
    "print(\"Example predictions of model: \" + \"\\n\" + \"x:\" + str(example_1) + \" y:\" + str(model.predict(example_1)) + \"\\n\"\n",
    "      + \"x:\" + str(example_2) + \" y:\" + str(model.predict(example_2)) + \"\\n\" + \"x:\" + str(example_3) + \" y:\" + str(model.predict(example_3)))\n",
    "print(\"\\n if these examples are predicted badly, the model might only perform well on training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### Intrinsic evaluation of embeddings\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------] 1.4% 23.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.7% 61.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==------------------------------------------------] 5.9% 98.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====----------------------------------------------] 8.2% 136.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 10.5% 174.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 12.7% 211.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 15.0% 249.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.2% 286.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 19.5% 324.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========----------------------------------------] 21.8% 362.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 24.0% 399.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 26.3% 437.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============------------------------------------] 28.6% 474.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============-----------------------------------] 30.8% 512.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================----------------------------------] 33.1% 550.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 35.4% 587.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================--------------------------------] 37.6% 625.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 39.9% 663.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 42.2% 700.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 44.4% 738.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 46.7% 776.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 48.9% 813.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 51.2% 851.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.5% 889.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 55.7% 926.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================---------------------] 58.0% 964.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 60.3% 1002.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================-------------------] 62.5% 1039.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================------------------] 64.8% 1077.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================-----------------] 67.1% 1115.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 69.3% 1152.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================================---------------] 71.6% 1190.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 73.9% 1228.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================------------] 76.1% 1265.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================-----------] 78.4% 1303.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================----------] 80.6% 1340.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================================---------] 82.9% 1378.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 85.1% 1415.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================================-------] 87.4% 1453.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================------] 89.7% 1491.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.9% 1528.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================================---] 94.2% 1566.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 96.5% 1604.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 98.0% 1629.5/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional libraries from students\n",
    "from gensim.models import Word2Vec\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from torchmetrics import SpearmanCorrCoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = []      \n",
    "    isFirstLine = True\n",
    "    for line in open('wordsim353_dataset.csv'):\n",
    "        if isFirstLine:\n",
    "            isFirstLine = False\n",
    "            continue\n",
    "        indices = [i for i, x in enumerate(line) if x == \",\"]\n",
    "        w1 = line[:indices[0]]\n",
    "        w2 = line[indices[0]+1:indices[1]]\n",
    "        mean = float(line[indices[1]+1:].rstrip())\n",
    "        # print(f\"w1: {w1}, w2: {w2}, mean: {mean}\")\n",
    "        data.append((w1, w2, mean))\n",
    "    return data\n",
    "\n",
    "# data = load_data()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes as input two words and computes the cosine similarity between them.\n",
    "You do not need to implement the cosine similarity calculation from scratch. Feel free to use any Python library.\n",
    "Remeber to ignore any pairs where at least one word is absent in the corpus. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    # TODO: check missing words\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    output = cos(word1, word2)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(w):\n",
    "    return torch.from_numpy(wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_scores(data):\n",
    "    # data = load_data()\n",
    "    scores = []\n",
    "    \n",
    "    # # scores = torch.tensor([])\n",
    "\n",
    "    for w1, w2, mean in data:\n",
    "        # empty words\n",
    "        if not w1 or not w2:\n",
    "            continue\n",
    "\n",
    "        # word2vec as tensors for cos_score\n",
    "        w1_vec = torch.from_numpy(wv[w1])\n",
    "        w2_vec = torch.from_numpy(wv[w2])\n",
    "        cos_score = similarity(w1_vec, w2_vec)\n",
    "        \n",
    "        scores.append([w1, w2, cos_score.item()])\n",
    "        # # scores = torch.cat((prev_word, word, cos_score.reshape(1)))\n",
    "    return sorted(scores, key=lambda entry: entry[2])\n",
    "# sim_scores = compute_similarity_scores(load_data())\n",
    "# print(len(sim_scores))\n",
    "# print(sim_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word pairs in the list based on the human judgement scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_judgement_scores(data):\n",
    "    # data = load_data()\n",
    "    return sorted(data, key=lambda entry: entry[2])\n",
    "# human_scores = human_judgement_scores(loaddata())\n",
    "# print(human_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute spearman rank correlation between the two ranked lists obtained in the previous two steps.\n",
    "You do not need to implement the spearman rank correlation calculation from scratch. Feel free to use any Python library. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 'sex', 6.77), ('tiger', 'cat', 7.35), ('tiger', 'tiger', 10.0), ('book', 'paper', 7.46), ('computer', 'keyboard', 7.62), ('computer', 'internet', 7.58), ('plane', 'car', 5.77), ('train', 'car', 6.31), ('telephone', 'communication', 7.5), ('television', 'radio', 6.77), ('media', 'radio', 7.42), ('drug', 'abuse', 6.85), ('bread', 'butter', 6.19), ('cucumber', 'potato', 5.92), ('doctor', 'nurse', 7.0), ('professor', 'doctor', 6.62), ('student', 'professor', 6.81), ('smart', 'student', 4.62), ('smart', 'stupid', 5.81), ('company', 'stock', 7.08), ('stock', 'market', 8.08), ('stock', 'phone', 1.62), ('stock', 'CD', 1.31), ('stock', 'jaguar', 0.92), ('stock', 'egg', 1.81), ('fertility', 'egg', 6.69), ('stock', 'live', 3.73), ('stock', 'life', 0.92), ('book', 'library', 7.46), ('bank', 'money', 8.12), ('wood', 'forest', 7.73), ('money', 'cash', 9.15), ('professor', 'cucumber', 0.31), ('king', 'cabbage', 0.23), ('king', 'queen', 8.58), ('king', 'rook', 5.92), ('bishop', 'rabbi', 6.69), ('Jerusalem', 'Israel', 8.46), ('Jerusalem', 'Palestinian', 7.65), ('holy', 'sex', 1.62), ('fuck', 'sex', 9.44), ('Maradona', 'football', 8.62), ('football', 'soccer', 9.03), ('football', 'basketball', 6.81), ('football', 'tennis', 6.63), ('tennis', 'racket', 7.56), ('Arafat', 'peace', 6.73), ('Arafat', 'terror', 7.65), ('Arafat', 'Jackson', 2.5), ('law', 'lawyer', 8.38), ('movie', 'star', 7.38), ('movie', 'popcorn', 6.19), ('movie', 'critic', 6.73), ('movie', 'theater', 7.92), ('physics', 'proton', 8.12), ('physics', 'chemistry', 7.35), ('space', 'chemistry', 4.88), ('alcohol', 'chemistry', 5.54), ('vodka', 'gin', 8.46), ('vodka', 'brandy', 8.13), ('drink', 'car', 3.04), ('drink', 'ear', 1.31), ('drink', 'mouth', 5.96), ('drink', 'eat', 6.87), ('baby', 'mother', 7.85), ('drink', 'mother', 2.65), ('car', 'automobile', 8.94), ('gem', 'jewel', 8.96), ('journey', 'voyage', 9.29), ('boy', 'lad', 8.83), ('coast', 'shore', 9.1), ('asylum', 'madhouse', 8.87), ('magician', 'wizard', 9.02), ('midday', 'noon', 9.29), ('furnace', 'stove', 8.79), ('food', 'fruit', 7.52), ('bird', 'cock', 7.1), ('bird', 'crane', 7.38), ('tool', 'implement', 6.46), ('brother', 'monk', 6.27), ('crane', 'implement', 2.69), ('lad', 'brother', 4.46), ('journey', 'car', 5.85), ('monk', 'oracle', 5.0), ('cemetery', 'woodland', 2.08), ('food', 'rooster', 4.42), ('coast', 'hill', 4.38), ('forest', 'graveyard', 1.85), ('shore', 'woodland', 3.08), ('monk', 'slave', 0.92), ('coast', 'forest', 3.15), ('lad', 'wizard', 0.92), ('chord', 'smile', 0.54), ('glass', 'magician', 2.08), ('noon', 'string', 0.54), ('rooster', 'voyage', 0.62), ('money', 'dollar', 8.42), ('money', 'cash', 9.08), ('money', 'currency', 9.04), ('money', 'wealth', 8.27), ('money', 'property', 7.57), ('money', 'possession', 7.29), ('money', 'bank', 8.5), ('money', 'deposit', 7.73), ('money', 'withdrawal', 6.88), ('money', 'laundering', 5.65), ('money', 'operation', 3.31), ('tiger', 'jaguar', 8.0), ('tiger', 'feline', 8.0), ('tiger', 'carnivore', 7.08), ('tiger', 'mammal', 6.85), ('tiger', 'animal', 7.0), ('tiger', 'organism', 4.77), ('tiger', 'fauna', 5.62), ('tiger', 'zoo', 5.87), ('psychology', 'psychiatry', 8.08), ('psychology', 'anxiety', 7.0), ('psychology', 'fear', 6.85), ('psychology', 'depression', 7.42), ('psychology', 'clinic', 6.58), ('psychology', 'doctor', 6.42), ('psychology', 'Freud', 8.21), ('psychology', 'mind', 7.69), ('psychology', 'health', 7.23), ('psychology', 'science', 6.71), ('psychology', 'discipline', 5.58), ('psychology', 'cognition', 7.48), ('planet', 'star', 8.45), ('planet', 'constellation', 8.06), ('planet', 'moon', 8.08), ('planet', 'sun', 8.02), ('planet', 'galaxy', 8.11), ('planet', 'space', 7.92), ('planet', 'astronomer', 7.94), ('precedent', 'example', 5.85), ('precedent', 'information', 3.85), ('precedent', 'cognition', 2.81), ('precedent', 'law', 6.65), ('precedent', 'collection', 2.5), ('precedent', 'group', 1.77), ('precedent', 'antecedent', 6.04), ('cup', 'coffee', 6.58), ('cup', 'tableware', 6.85), ('cup', 'article', 2.4), ('cup', 'artifact', 2.92), ('cup', 'object', 3.69), ('cup', 'entity', 2.15), ('cup', 'drink', 7.25), ('cup', 'food', 5.0), ('cup', 'substance', 1.92), ('cup', 'liquid', 5.9), ('jaguar', 'cat', 7.42), ('jaguar', 'car', 7.27), ('energy', 'secretary', 1.81), ('secretary', 'senate', 5.06), ('energy', 'laboratory', 5.09), ('computer', 'laboratory', 6.78), ('weapon', 'secret', 6.06), ('FBI', 'fingerprint', 6.94), ('FBI', 'investigation', 8.31), ('investigation', 'effort', 4.59), ('Mars', 'water', 2.94), ('Mars', 'scientist', 5.63), ('news', 'report', 8.16), ('canyon', 'landscape', 7.53), ('image', 'surface', 4.56), ('discovery', 'space', 6.34), ('water', 'seepage', 6.56), ('sign', 'recess', 2.38), ('Wednesday', 'news', 2.22), ('mile', 'kilometer', 8.66), ('computer', 'news', 4.47), ('territory', 'surface', 5.34), ('atmosphere', 'landscape', 3.69), ('president', 'medal', 3.0), ('war', 'troops', 8.13), ('record', 'number', 6.31), ('skin', 'eye', 6.22), ('Japanese', 'American', 6.5), ('theater', 'history', 3.91), ('volunteer', 'motto', 2.56), ('prejudice', 'recognition', 3.0), ('decoration', 'valor', 5.63), ('century', 'year', 7.59), ('century', 'nation', 3.16), ('delay', 'racism', 1.19), ('delay', 'news', 3.31), ('minister', 'party', 6.63), ('peace', 'plan', 4.75), ('minority', 'peace', 3.69), ('attempt', 'peace', 4.25), ('government', 'crisis', 6.56), ('deployment', 'departure', 4.25), ('deployment', 'withdrawal', 5.88), ('energy', 'crisis', 5.94), ('announcement', 'news', 7.56), ('announcement', 'effort', 2.75), ('stroke', 'hospital', 7.03), ('disability', 'death', 5.47), ('victim', 'emergency', 6.47), ('treatment', 'recovery', 7.91), ('journal', 'association', 4.97), ('doctor', 'personnel', 5.0), ('doctor', 'liability', 5.19), ('liability', 'insurance', 7.03), ('school', 'center', 3.44), ('reason', 'hypertension', 2.31), ('reason', 'criterion', 5.91), ('hundred', 'percent', 7.38), ('Harvard', 'Yale', 8.13), ('hospital', 'infrastructure', 4.63), ('death', 'row', 5.25), ('death', 'inmate', 5.03), ('lawyer', 'evidence', 6.69), ('life', 'death', 7.88), ('life', 'term', 4.5), ('word', 'similarity', 4.75), ('board', 'recommendation', 4.47), ('governor', 'interview', 3.25), ('OPEC', 'country', 5.63), ('peace', 'atmosphere', 3.69), ('peace', 'insurance', 2.94), ('territory', 'kilometer', 5.28), ('travel', 'activity', 5.0), ('competition', 'price', 6.44), ('consumer', 'confidence', 4.13), ('consumer', 'energy', 4.75), ('problem', 'airport', 2.38), ('car', 'flight', 4.94), ('credit', 'card', 8.06), ('credit', 'information', 5.31), ('hotel', 'reservation', 8.03), ('grocery', 'money', 5.94), ('registration', 'arrangement', 6.0), ('arrangement', 'accommodation', 5.41), ('month', 'hotel', 1.81), ('type', 'kind', 8.97), ('arrival', 'hotel', 6.0), ('bed', 'closet', 6.72), ('closet', 'clothes', 8.0), ('situation', 'conclusion', 4.81), ('situation', 'isolation', 3.88), ('impartiality', 'interest', 5.16), ('direction', 'combination', 2.25), ('street', 'place', 6.44), ('street', 'avenue', 8.88), ('street', 'block', 6.88), ('street', 'children', 4.94), ('listing', 'proximity', 2.56), ('listing', 'category', 6.38), ('cell', 'phone', 7.81), ('production', 'hike', 1.75), ('benchmark', 'index', 4.25), ('media', 'trading', 3.88), ('media', 'gain', 2.88), ('dividend', 'payment', 7.63), ('dividend', 'calculation', 6.48), ('calculation', 'computation', 8.44), ('currency', 'market', 7.5), ('OPEC', 'oil', 8.59), ('oil', 'stock', 6.34), ('announcement', 'production', 3.38), ('announcement', 'warning', 6.0), ('profit', 'warning', 3.88), ('profit', 'loss', 7.63), ('dollar', 'yen', 7.78), ('dollar', 'buck', 9.22), ('dollar', 'profit', 7.38), ('dollar', 'loss', 6.09), ('computer', 'software', 8.5), ('network', 'hardware', 8.31), ('phone', 'equipment', 7.13), ('equipment', 'maker', 5.91), ('luxury', 'car', 6.47), ('five', 'month', 3.38), ('report', 'gain', 3.63), ('investor', 'earning', 7.13), ('liquid', 'water', 7.89), ('baseball', 'season', 5.97), ('game', 'victory', 7.03), ('game', 'team', 7.69), ('marathon', 'sprint', 7.47), ('game', 'series', 6.19), ('game', 'defeat', 6.97), ('seven', 'series', 3.56), ('seafood', 'sea', 7.47), ('seafood', 'food', 8.34), ('seafood', 'lobster', 8.7), ('lobster', 'food', 7.81), ('lobster', 'wine', 5.7), ('food', 'preparation', 6.22), ('video', 'archive', 6.34), ('start', 'year', 4.06), ('start', 'match', 4.47), ('game', 'round', 5.97), ('boxing', 'round', 7.61), ('championship', 'tournament', 8.36), ('fighting', 'defeating', 7.41), ('line', 'insurance', 2.69), ('day', 'summer', 3.94), ('summer', 'drought', 7.16), ('summer', 'nature', 5.63), ('day', 'dawn', 7.53), ('nature', 'environment', 8.31), ('environment', 'ecology', 8.81), ('nature', 'man', 6.25), ('man', 'woman', 8.3), ('man', 'governor', 5.25), ('murder', 'manslaughter', 8.53), ('soap', 'opera', 7.94), ('opera', 'performance', 6.88), ('life', 'lesson', 5.94), ('focus', 'life', 4.06), ('production', 'crew', 6.25), ('television', 'film', 7.72), ('lover', 'quarrel', 6.19), ('viewer', 'serial', 2.97), ('possibility', 'girl', 1.94), ('population', 'development', 3.75), ('morality', 'importance', 3.31), ('morality', 'marriage', 3.69), ('Mexico', 'Brazil', 7.44), ('gender', 'equality', 6.41), ('change', 'attitude', 5.44), ('family', 'planning', 6.25), ('opera', 'industry', 2.63), ('sugar', 'approach', 0.88), ('practice', 'institution', 3.19), ('ministry', 'culture', 4.69), ('problem', 'challenge', 6.75), ('size', 'prominence', 5.31), ('country', 'citizen', 7.31), ('planet', 'people', 5.75), ('development', 'issue', 3.97), ('experience', 'music', 3.47), ('music', 'project', 3.63), ('glass', 'metal', 5.56), ('aluminum', 'metal', 7.83), ('chance', 'credibility', 3.88), ('exhibit', 'memorabilia', 5.31), ('concert', 'virtuoso', 6.81), ('rock', 'jazz', 7.59), ('museum', 'theater', 7.19), ('observation', 'architecture', 4.38), ('space', 'world', 6.53), ('preservation', 'world', 6.19), ('admission', 'ticket', 7.69), ('shower', 'thunderstorm', 6.31), ('shower', 'flood', 6.03), ('weather', 'forecast', 8.34), ('disaster', 'area', 6.25), ('governor', 'office', 6.34), ('architecture', 'century', 3.78)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_spearman():\n",
    "    data = load_data()\n",
    "    print(data)\n",
    "\n",
    "    sim_score = np.array(compute_similarity_scores(data))\n",
    "    human_score = np.array(human_judgement_scores(data))\n",
    "\n",
    "    # use third column of dataset (mean/cos_similarity)\n",
    "    preds = torch.from_numpy(sim_score[:, 2].astype(float))\n",
    "    target = torch.from_numpy(human_score[:, 2].astype(float))\n",
    "\n",
    "    spearman = SpearmanCorrCoef()\n",
    "    return spearman(preds, target)\n",
    "    \n",
    "compute_spearman()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding based clasifier\n",
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n",
    "\n",
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fun',\n",
       " ',',\n",
       " 'Wasabi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'start',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n",
    "    \n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    res = np.mean([wv[word] for word in review if word in wv], axis=0)\n",
    "    # print(res)\n",
    "    # print(res.shape)\n",
    "    # print(review)\n",
    "    if np.isnan(res).any():\n",
    "        return np.zeros(300)\n",
    "    return res\n",
    "\n",
    "# [word1, word2, word3]\n",
    "# [wv1, wv2,]\n",
    "\n",
    "# generate_embedding(test_X[0])\n",
    "# generate_embedding(train_X[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.fc_1 = nn.Linear(self.input_size, self.hidden_1_size)\n",
    "        self.fc_2 = nn.Linear(self.hidden_1_size, self.hidden_2_size)\n",
    "        self.fc_3 = nn.Linear(self.hidden_2_size, self.output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.fc_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_1 = self.relu(self.fc_1(x))\n",
    "        h_2 = self.relu(self.fc_2(h_1))\n",
    "        y = self.fc_3(h_2)\n",
    "        return self.sigmoid(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class for efficiently enumerating over the dataset. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sent_data(Dataset):\n",
    "    def __init__(self, data_points, class_labels):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data_points\n",
    "        self.labels = class_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        d = torch.FloatTensor(generate_embedding(self.data[index]))\n",
    "        l = torch.FloatTensor([self.labels[index]])\n",
    "        return d,l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to calculate the accuracy\n",
    "def calc_accuracy(gts, preds):\n",
    "    correct = sum(1 for tc, pred in zip(gts, preds) if tc == pred)\n",
    "    total = len(gts)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "# Adopt your code to incorporate mini-batch training\n",
    "# Use cross-entropy as your loss function\n",
    "def train(model, train_data, val_data, batch_size, epochs=5, learning_rate=0.001):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    lossfn = nn.BCELoss()\n",
    "    best_accuracy = 0.0\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        for d, l in train_dataloader:\n",
    "            out = model(d)\n",
    "            # print(d)\n",
    "            # print(len(d))\n",
    "            # print(out)\n",
    "            # print(out.shape)\n",
    "            # print(l)\n",
    "            # print(l.shape)\n",
    "            # print(len(l))\n",
    "            loss =  lossfn(out, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        true_classes = []\n",
    "        predicted_classes = []\n",
    "        for d, l in val_dataloader:\n",
    "            out = model(d)\n",
    "            true_classes.extend(l.squeeze(1).tolist())\n",
    "            predictions = torch.where(out > 0.5, 1, 0) # for two classes and out in (0,1) this should be enough for class probabilities? \n",
    "            predicted_classes.extend(predictions.squeeze(1).tolist())\n",
    "            \n",
    "        # Calculate the Accuracy\n",
    "        accuracy = calc_accuracy(true_classes, predicted_classes)\n",
    "\n",
    "        #Check whether current model is better than best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model.state_dict()\n",
    "            \n",
    "    # Save the best model\n",
    "    torch.save(best_model, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set and report the test accuracy. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    model.eval()\n",
    "    test_dataloader = DataLoader(test_data)\n",
    "    true_classes = []\n",
    "    predicted_classes = []\n",
    "    for d,l in test_dataloader:\n",
    "        out = model(d)\n",
    "        true_classes.extend(l)\n",
    "        predictions = torch.where(out > 0.5, 1, 0)\n",
    "        predicted_classes.extend(predictions.squeeze(1).tolist())\n",
    "        \n",
    "    # Calculate the Accuracy\n",
    "    accuracy = calc_accuracy(true_classes, predicted_classes)\n",
    "\n",
    "    print(\"The accuracy of the best model on the test set is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:25<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the best model on the test set is: 0.7896760021965953\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER: Maybe implement GPU training for faster execution\n",
    "train_data = sent_data(train_X, train_y)\n",
    "val_data = sent_data(val_X, val_y)\n",
    "test_data = sent_data(test_X, test_y)\n",
    "model = Classifier(300, 100, 50, 1)\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "train(model, train_data, val_data, batch_size, epochs)\n",
    "best_model_state = torch.load('best_model.pth')\n",
    "model.load_state_dict(best_model_state)\n",
    "evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.6015e-26], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.0001], grad_fn=<SigmoidBackward0>)\n",
      "tensor([2.7837e-19], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "print(model.forward(torch.Tensor(wv[\"good\"])))\n",
    "print(model.forward(torch.Tensor(wv[\"awful\"])))\n",
    "print(model.forward(torch.Tensor(wv[\"eventually\"])))\n",
    "print(model.forward(torch.Tensor(wv[\"boring\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
