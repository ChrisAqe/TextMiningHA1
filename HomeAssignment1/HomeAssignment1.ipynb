{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1\n",
    "Due by 8th May, 2024 at 23:59 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "We want to create a 2 layer NN, which means we want to calculate  $y = W_2 * ReLU(W_1 * x + b_1) + b_2$\n",
    "\n",
    "Complete the TODOs below to create such a NN.\n",
    "\n",
    "Since you will be needing to compute the gradients w.r.t. all parameters, you may look into online resources for help. Please cite or link any online recources you do use.\n",
    "\n",
    "You are allowed to change any existing parts, however the code has to remain easy to understand and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "        \n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise ReLU application\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    # TODO: Write a function given a numpy array that calculates the gradient of the ReLU function w.r.t. `x`\n",
    "    # TODO: Also write the derivation of the gradient in the PDF file In the implementation you may simply use the final derivation.\n",
    "    # Hint: The function should return a numpy array of the same dimension that `x` has, but only containing 0 or 1\n",
    "    arr = np.zeros(x.shape)\n",
    "    return np.greater(x, arr).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyNeuralNet:\n",
    "    \n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        # TODO: Randomly initialize the weight matrices W_1, W_2 and biases b_1, b_2\n",
    "        # Hint: use np.random.randn() and make sure to correctly set the dimensions \n",
    "\n",
    "        # Scale random sample with 0.01 according to lecture\n",
    "        self.W_1 = 0.01 * np.random.randn(self.dim_in, self.dim_hidden)\n",
    "        self.b_1 = 0.01 * np.random.randn(self.dim_hidden)\n",
    "        self.W_2 = 0.01 * np.random.randn(self.dim_hidden, self.dim_out)\n",
    "        self.b_2 = 0.01 * np.random.randn(self.dim_out)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of the neural network for the given x.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): predicted output for `x`\n",
    "        \"\"\"\n",
    "        # TODO: Calculate output self.out\n",
    "        # Safe intermediate results as cache for later backpropagation\n",
    "        self.h_1 = np.dot(x, self.W_1) + self.b_1\n",
    "        self.h_1_act = relu(self.h_1)\n",
    "        self.out = np.dot(self.h_1_act, self.W_2) + self.b_2\n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the Mean-Squared Error and returns the gradients w.r.t. to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array with shape (self.dim_in, n)\n",
    "            y (np.ndarray): ground truth value numpy array with shape (self.dim_out, n)\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Mean-Squared-Error between predicted value on input points and ground truth value\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1   \n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2  \n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1   \n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2   \n",
    "        \"\"\"\n",
    "        # TODO: Calculate the loss (Mean-Squared-Error)\n",
    "        # Hint: use np.square() and np.mean()\n",
    "\n",
    "        y_pred = self.predict(x)\n",
    "        loss = np.mean(np.square(y_pred - y))\n",
    "        \n",
    "        # TODO: Calculate all gradients w.r.t to the parameters\n",
    "        # Hint: You need to calculate the gradients for each of the parameters by hand\n",
    "        # TODO: Also write the derivation of the gradient in the PDF file. In the implementation you may simply use the final derivation.\n",
    "\n",
    "        loss_derived = 2 * (self.out - y) / len(y)\n",
    "        \n",
    "        h_1_grad = np.dot(loss_derived, self.W_2.T) * relu_grad(self.h_1)\n",
    "        \n",
    "        W_2_grad = np.dot(self.h_1_act.T, loss_derived)\n",
    "        b_2_grad = np.sum(loss_derived, axis=0)\n",
    "        W_1_grad = np.dot(x.T, h_1_grad)\n",
    "        b_1_grad = np.sum(h_1_grad, axis=0)\n",
    "\n",
    "        return loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad\n",
    "         \n",
    "    def train(self, x, y, lr=0.001, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input values\n",
    "            y (np.ndarray): ground truth values\n",
    "            lr (float): learning rate, default: 0.001\n",
    "            epochs (int): number of epochs to train, default: 1000\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Return the loss achieved after all epochs\n",
    "        \"\"\"\n",
    "        # TODO: Keep track of the loss\n",
    "        loss_history = []\n",
    "        n = len(x)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # shuffle dataset\n",
    "            idx = np.arange(n)\n",
    "            np.random.shuffle(idx)\n",
    "            x_shuffled = x[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            for i in range(n):\n",
    "                dp_x = x_shuffled[i]\n",
    "                dp_y = y_shuffled[i]\n",
    "                \n",
    "                loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad = self.loss(dp_x.reshape(1,-1), dp_y.reshape(1,-1))\n",
    "                \n",
    "                self.W_1 -= lr * W_1_grad\n",
    "                self.W_2 -= lr * W_2_grad\n",
    "                self.b_1 -= lr * b_1_grad\n",
    "                self.b_2 -= lr * b_2_grad\n",
    "\n",
    "                loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            # print mean loss of dataset every 10% of epochs\n",
    "            e = int(epochs / 10)\n",
    "            if epoch % e == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {np.mean(loss_history[-n:])}')\n",
    "\n",
    "        \n",
    "        # TODO: Plot the loss history and return the loss achieved after the final epoch\n",
    "        # Plot the loss history after every epoch. Returned is the mean loss of the last epoch.\n",
    "        loss_per_epoch = [np.mean(loss_history[i*n:(i+1)*n]) for i in range(epochs)]\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.show()\n",
    "\n",
    "        return loss_per_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3048135152827944\n",
      "Epoch 100, Loss: 0.004766474955605456\n",
      "Epoch 200, Loss: 0.001929123905124359\n",
      "Epoch 300, Loss: 0.0011586061105498446\n",
      "Epoch 400, Loss: 0.0008842943399885283\n",
      "Epoch 500, Loss: 0.0007160753823985639\n",
      "Epoch 600, Loss: 0.0005888245134945728\n",
      "Epoch 700, Loss: 0.000526121102732088\n",
      "Epoch 800, Loss: 0.0005047205437658141\n",
      "Epoch 900, Loss: 0.0004532285108616859\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after final epoch: 0.00043452530197944737\n",
      "Example predictions of model: \n",
      "x:0.4 y:[[0.1717219]]\n",
      "x:0.5 y:[[0.24908155]]\n",
      "x:0.7 y:[[0.49592807]]\n",
      "\n",
      " if these examples are predicted badly, the model might only perform well on training set\n"
     ]
    }
   ],
   "source": [
    "# We test the model created above on the simple function y = x^2\n",
    "\n",
    "model = NumPyNeuralNet(1, 30, 1)\n",
    "\n",
    "# Create a randomly distributed array of 1000 values between 0 and 1\n",
    "x_train = 1 * np.random.randn(1000, 1)\n",
    "# Create ground truth by calculating x*x\n",
    "y_train = x_train * x_train\n",
    "\n",
    "# Train for default epochs\n",
    "loss = model.train(x_train, y_train)\n",
    "print(\"loss after final epoch: \" + str(loss))\n",
    "example_1 = 0.4\n",
    "example_2 = 0.5\n",
    "example_3 = 0.7\n",
    "print(\"Example predictions of model: \" + \"\\n\" + \"x:\" + str(example_1) + \" y:\" + str(model.predict(example_1)) + \"\\n\"\n",
    "      + \"x:\" + str(example_2) + \" y:\" + str(model.predict(example_2)) + \"\\n\" + \"x:\" + str(example_3) + \" y:\" + str(model.predict(example_3)))\n",
    "print(\"\\n if these examples are predicted badly, the model might only perform well on training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### Intrinsic evaluation of embeddings\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional libraries from students\n",
    "from gensim.models import Word2Vec\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from torchmetrics import SpearmanCorrCoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nZelle zum testen/rumprobieren\\n\\n# discard word pairs where at least one word is absent \\ndef test():\\n    scores = []\\n    # scores = torch.tensor([])\\n\\n    cos = nn.CosineSimilarity(dim=0)\\n    prev_word = \"\"\\n    for index, word in enumerate(wv.index_to_key):\\n        # empty words\\n        if not prev_word:\\n            prev_word = word\\n            continue\\n\\n        # word2vec as tensors\\n        # w1 = torch.from_numpy(np.array([wv[prev_word]]))\\n        # w2 = torch.from_numpy(np.array([wv[word]]))\\n        w1 = torch.from_numpy(wv[prev_word])\\n        w2 = torch.from_numpy(wv[word])\\n        \\n\\n        # print(w1.shape)\\n        # print(w2.shape)\\n        cos_score = cos(w1, w2)\\n        # print(cosScore)\\n        # print(w1)\\n        # print(w2)\\n        scores.append([prev_word, word, cos_score])\\n        # scores = torch.cat((prev_word, word, cos_score.reshape(1)))\\n        # print(f\"prev_word: {prev_word}, word: {word}, with score: {cos_score}\")\\n        if index >= 10:\\n            break\\n        # print(scores)\\n        prev_word = word\\n    # print(scores)\\n    scores_sorted = sorted(scores, key=lambda a_entry: a_entry[2]) \\n    # print(a)\\n\\n    return scores_sorted\\n            \\n\\n# scores = test()\\n# print(scores)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Zelle zum testen/rumprobieren\n",
    "\n",
    "# discard word pairs where at least one word is absent \n",
    "def test():\n",
    "    scores = []\n",
    "    # scores = torch.tensor([])\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    prev_word = \"\"\n",
    "    for index, word in enumerate(wv.index_to_key):\n",
    "        # empty words\n",
    "        if not prev_word:\n",
    "            prev_word = word\n",
    "            continue\n",
    "\n",
    "        # word2vec as tensors\n",
    "        # w1 = torch.from_numpy(np.array([wv[prev_word]]))\n",
    "        # w2 = torch.from_numpy(np.array([wv[word]]))\n",
    "        w1 = torch.from_numpy(wv[prev_word])\n",
    "        w2 = torch.from_numpy(wv[word])\n",
    "        \n",
    "\n",
    "        # print(w1.shape)\n",
    "        # print(w2.shape)\n",
    "        cos_score = cos(w1, w2)\n",
    "        # print(cosScore)\n",
    "        # print(w1)\n",
    "        # print(w2)\n",
    "        scores.append([prev_word, word, cos_score])\n",
    "        # scores = torch.cat((prev_word, word, cos_score.reshape(1)))\n",
    "        # print(f\"prev_word: {prev_word}, word: {word}, with score: {cos_score}\")\n",
    "        if index >= 10:\n",
    "            break\n",
    "        # print(scores)\n",
    "        prev_word = word\n",
    "    # print(scores)\n",
    "    scores_sorted = sorted(scores, key=lambda a_entry: a_entry[2]) \n",
    "    # print(a)\n",
    "\n",
    "    return scores_sorted\n",
    "            \n",
    "\n",
    "# scores = test()\n",
    "# print(scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = []      \n",
    "    isFirstLine = True\n",
    "    for line in open('wordsim353_dataset.csv'):\n",
    "        if isFirstLine:\n",
    "            isFirstLine = False\n",
    "            continue\n",
    "        indices = [i for i, x in enumerate(line) if x == \",\"]\n",
    "        w1 = line[:indices[0]]\n",
    "        w2 = line[indices[0]+1:indices[1]]\n",
    "        mean = float(line[indices[1]+1:].rstrip())\n",
    "        # print(f\"w1: {w1}, w2: {w2}, mean: {mean}\")\n",
    "        data.append((w1, w2, mean))\n",
    "    return data\n",
    "\n",
    "# data = load_data()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes as input two words and computes the cosine similarity between them.\n",
    "You do not need to implement the cosine similarity calculation from scratch. Feel free to use any Python library.\n",
    "Remeber to ignore any pairs where at least one word is absent in the corpus. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    output = cos(word1, word2)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(w):\n",
    "    return torch.from_numpy(wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_scores(data):\n",
    "    # data = load_data()\n",
    "    scores = []\n",
    "    \n",
    "    # # scores = torch.tensor([])\n",
    "\n",
    "    for w1, w2, mean in data:\n",
    "        # empty words\n",
    "        if not w1 or not w2:\n",
    "            continue\n",
    "\n",
    "        # word2vec as tensors for cos_score\n",
    "        w1_vec = torch.from_numpy(wv[w1])\n",
    "        w2_vec = torch.from_numpy(wv[w2])\n",
    "        cos_score = similarity(w1_vec, w2_vec)\n",
    "        \n",
    "        scores.append([w1, w2, cos_score])\n",
    "        # # scores = torch.cat((prev_word, word, cos_score.reshape(1)))\n",
    "    return sorted(scores, key=lambda entry: entry[2])\n",
    "# sim_scores = compute_similarity_scores(load_data())\n",
    "# print(len(sim_scores))\n",
    "# print(sim_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word pairs in the list based on the human judgement scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_judgement_scores(data):\n",
    "    # data = load_data()\n",
    "    return sorted(data, key=lambda entry: entry[2])\n",
    "# human_scores = human_judgement_scores(loaddata())\n",
    "# print(human_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute spearman rank correlation between the two ranked lists obtained in the previous two steps.\n",
    "You do not need to implement the spearman rank correlation calculation from scratch. Feel free to use any Python library. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearman():\n",
    "    # data = load_data()\n",
    "    # print(data)\n",
    "    # spearman = SpearmanCorrCoef()\n",
    "    # # spearman(preds, target)\n",
    "    # spearman(var1, var2)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding based clasifier\n",
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n",
    "\n",
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n",
    "    \n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    # return embedding\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class for efficiently enumerating over the dataset. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sent_data(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopt your code to incorporate mini-batch training\n",
    "# Use cross-entropy as your loss function\n",
    "def train(model, train_data, val_data, epochs=5, learning_rate=0.001):\n",
    "    # write your code snippet here\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set and report the test accuracy. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
