{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1\n",
    "Due by 8th May, 2024 at 23:59 CEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChristoph Hlava - 10027820\\nDominik Rittner - 10019667\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Christoph Hlava - 10027820\n",
    "Dominik Rittner - 10019667\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "We want to create a 2 layer NN, which means we want to calculate  $y = W_2 * ReLU(W_1 * x + b_1) + b_2$\n",
    "\n",
    "Complete the TODOs below to create such a NN.\n",
    "\n",
    "Since you will be needing to compute the gradients w.r.t. all parameters, you may look into online resources for help. Please cite or link any online recources you do use.\n",
    "\n",
    "You are allowed to change any existing parts, however the code has to remain easy to understand and well documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.ndarray): numpy array with shape (m, n) where m is the number of dimensions and n is the number of points\n",
    "        \n",
    "    Returns:\n",
    "        x' (np.ndarray): return value of the pointwise ReLU application\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    # TODO: Write a function given a numpy array that calculates the gradient of the ReLU function w.r.t. `x`\n",
    "    # TODO: Also write the derivation of the gradient in the PDF file In the implementation you may simply use the final derivation.\n",
    "    # Hint: The function should return a numpy array of the same dimension that `x` has, but only containing 0 or 1\n",
    "    arr = np.zeros(x.shape)\n",
    "    return np.greater(x, arr).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumPyNeuralNet:\n",
    "    \n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        # TODO: Randomly initialize the weight matrices W_1, W_2 and biases b_1, b_2\n",
    "        # Hint: use np.random.randn() and make sure to correctly set the dimensions \n",
    "\n",
    "        # Scale random sample with 0.01 according to lecture\n",
    "        self.W_1 = 0.01 * np.random.randn(self.dim_in, self.dim_hidden)\n",
    "        self.b_1 = 0.01 * np.random.randn(self.dim_hidden)\n",
    "        self.W_2 = 0.01 * np.random.randn(self.dim_hidden, self.dim_out)\n",
    "        self.b_2 = 0.01 * np.random.randn(self.dim_out)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of the neural network for the given x.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): predicted output for `x`\n",
    "        \"\"\"\n",
    "        # TODO: Calculate output self.out\n",
    "        # Safe intermediate results as cache for later backpropagation\n",
    "        self.h_1 = np.dot(x, self.W_1) + self.b_1\n",
    "        self.h_1_act = relu(self.h_1)\n",
    "        self.out = np.dot(self.h_1_act, self.W_2) + self.b_2\n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the Mean-Squared Error and returns the gradients w.r.t. to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input value numpy array with shape (self.dim_in, n)\n",
    "            y (np.ndarray): ground truth value numpy array with shape (self.dim_out, n)\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Mean-Squared-Error between predicted value on input points and ground truth value\n",
    "            W_1_grad (np.ndarray): gradient w.r.t W_1   \n",
    "            W_2_grad (np.ndarray): gradient w.r.t W_2  \n",
    "            b_1_grad (np.ndarray): gradient w.r.t b_1   \n",
    "            b_2_grad (np.ndarray): gradient w.r.t b_2   \n",
    "        \"\"\"\n",
    "        # TODO: Calculate the loss (Mean-Squared-Error)\n",
    "        # Hint: use np.square() and np.mean()\n",
    "\n",
    "        y_pred = self.predict(x)\n",
    "        loss = np.mean(np.square(y_pred - y))\n",
    "        \n",
    "        # TODO: Calculate all gradients w.r.t to the parameters\n",
    "        # Hint: You need to calculate the gradients for each of the parameters by hand\n",
    "        # TODO: Also write the derivation of the gradient in the PDF file. In the implementation you may simply use the final derivation.\n",
    "\n",
    "        loss_derived = 2 * (self.out - y) / len(y)\n",
    "        \n",
    "        h_1_grad = np.dot(loss_derived, self.W_2.T) * relu_grad(self.h_1)\n",
    "        \n",
    "        W_2_grad = np.dot(self.h_1_act.T, loss_derived)\n",
    "        b_2_grad = np.sum(loss_derived, axis=0)\n",
    "        W_1_grad = np.dot(x.T, h_1_grad)\n",
    "        b_1_grad = np.sum(h_1_grad, axis=0)\n",
    "\n",
    "        return loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad\n",
    "         \n",
    "    def train(self, x, y, lr=0.001, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "            x (np.ndarray): input values\n",
    "            y (np.ndarray): ground truth values\n",
    "            lr (float): learning rate, default: 0.001\n",
    "            epochs (int): number of epochs to train, default: 1000\n",
    "            \n",
    "        Returns:\n",
    "            loss (float): Return the loss achieved after all epochs\n",
    "        \"\"\"\n",
    "        # TODO: Keep track of the loss\n",
    "        loss_history = []\n",
    "        n = len(x)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # shuffle dataset\n",
    "            idx = np.arange(n)\n",
    "            np.random.shuffle(idx)\n",
    "            x_shuffled = x[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            for i in range(n):\n",
    "                dp_x = x_shuffled[i]\n",
    "                dp_y = y_shuffled[i]\n",
    "                \n",
    "                loss, W_1_grad, W_2_grad, b_1_grad, b_2_grad = self.loss(dp_x.reshape(1,-1), dp_y.reshape(1,-1))\n",
    "                \n",
    "                self.W_1 -= lr * W_1_grad\n",
    "                self.W_2 -= lr * W_2_grad\n",
    "                self.b_1 -= lr * b_1_grad\n",
    "                self.b_2 -= lr * b_2_grad\n",
    "\n",
    "                loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            # print mean loss of dataset every 10% of epochs\n",
    "            e = int(epochs / 10)\n",
    "            if epoch % e == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {np.mean(loss_history[-n:])}')\n",
    "\n",
    "        \n",
    "        # TODO: Plot the loss history and return the loss achieved after the final epoch\n",
    "        # Plot the loss history after every epoch. Returned is the mean loss of the last epoch.\n",
    "        loss_per_epoch = [np.mean(loss_history[i*n:(i+1)*n]) for i in range(epochs)]\n",
    "        plt.plot(loss_per_epoch)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.show()\n",
    "\n",
    "        return loss_per_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3666745753743754\n",
      "Epoch 100, Loss: 0.0036042473205988306\n",
      "Epoch 200, Loss: 0.0013828907632766091\n",
      "Epoch 300, Loss: 0.0008922379773640516\n",
      "Epoch 400, Loss: 0.0007103713863246147\n",
      "Epoch 500, Loss: 0.0006294838922891847\n",
      "Epoch 600, Loss: 0.0005601103028571541\n",
      "Epoch 700, Loss: 0.0005189778265737468\n",
      "Epoch 800, Loss: 0.0004907267323295599\n",
      "Epoch 900, Loss: 0.00046427101441351735\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after final epoch: 0.0004366728639815288\n",
      "Example predictions of model: \n",
      "x:0.4 y:[[0.17792394]]\n",
      "x:0.5 y:[[0.25826863]]\n",
      "x:0.7 y:[[0.4666888]]\n",
      "\n",
      " if these examples are predicted badly, the model might only perform well on training set\n"
     ]
    }
   ],
   "source": [
    "# We test the model created above on the simple function y = x^2\n",
    "\n",
    "model = NumPyNeuralNet(1, 30, 1)\n",
    "\n",
    "# Create a randomly distributed array of 1000 values between 0 and 1\n",
    "x_train = 1 * np.random.randn(1000, 1)\n",
    "# Create ground truth by calculating x*x\n",
    "y_train = x_train * x_train\n",
    "\n",
    "# Train for default epochs\n",
    "loss = model.train(x_train, y_train)\n",
    "print(\"loss after final epoch: \" + str(loss))\n",
    "example_1 = 0.4\n",
    "example_2 = 0.5\n",
    "example_3 = 0.7\n",
    "print(\"Example predictions of model: \" + \"\\n\" + \"x:\" + str(example_1) + \" y:\" + str(model.predict(example_1)) + \"\\n\"\n",
    "      + \"x:\" + str(example_2) + \" y:\" + str(model.predict(example_2)) + \"\\n\" + \"x:\" + str(example_3) + \" y:\" + str(model.predict(example_3)))\n",
    "print(\"\\n if these examples are predicted badly, the model might only perform well on training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "### Intrinsic evaluation of embeddings\n",
    "Word similarity task is often used as an intrinsic evaluation criteria. In the dataset file you will find a list of word pairs with their similarity scores as judged by humans. The task would be to judge how well are the word vectors aligned to human judgement. We will use word2vec embedding vectors trained on the google news corpus. (Ignore the pairs where at least one the words is absent in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional libraries from students\n",
    "from gensim.models import Word2Vec\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from torchmetrics import SpearmanCorrCoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = []      \n",
    "    isFirstLine = True\n",
    "    for line in open('wordsim353_dataset.csv'):\n",
    "        if isFirstLine:\n",
    "            isFirstLine = False\n",
    "            continue\n",
    "        indices = [i for i, x in enumerate(line) if x == \",\"]\n",
    "        w1 = line[:indices[0]]\n",
    "        w2 = line[indices[0]+1:indices[1]]\n",
    "        mean = float(line[indices[1]+1:].rstrip())\n",
    "        # print(f\"w1: {w1}, w2: {w2}, mean: {mean}\")\n",
    "        data.append((w1, w2, mean))\n",
    "    return data\n",
    "\n",
    "# data = load_data()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which takes as input two words and computes the cosine similarity between them.\n",
    "You do not need to implement the cosine similarity calculation from scratch. Feel free to use any Python library.\n",
    "Remeber to ignore any pairs where at least one word is absent in the corpus. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    # Missing words are taken care of in compute_similarity_scores\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    output = cos(word1, word2)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between all the word pairs in the list and sort them based on the similarity scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['preservation', 'world', -0.021470628678798676], ['precedent', 'cognition', -0.020569605752825737], ['investor', 'earning', 0.0034081414341926575], ['sugar', 'approach', 0.007720557972788811], ['production', 'hike', 0.018261179327964783], ['music', 'project', 0.02134612202644348], ['noon', 'string', 0.021654512733221054], ['crane', 'implement', 0.023186158388853073], ['delay', 'racism', 0.023803047835826874], ['dollar', 'loss', 0.026566721498966217], ['precedent', 'collection', 0.03283146768808365], ['cup', 'artifact', 0.033699166029691696], ['cup', 'entity', 0.034792210906744], ['stock', 'jaguar', 0.03606690466403961], ['glass', 'magician', 0.0372249037027359], ['discovery', 'space', 0.04125640168786049], ['opera', 'industry', 0.04291349649429321], ['stock', 'live', 0.04447175934910774], ['sign', 'recess', 0.04805115610361099], ['money', 'possession', 0.0531826913356781], ['money', 'withdrawal', 0.05602632462978363], ['professor', 'cucumber', 0.05661814659833908], ['possibility', 'girl', 0.0600377656519413], ['doctor', 'liability', 0.06116040050983429], ['attempt', 'peace', 0.061378709971904755], ['rooster', 'voyage', 0.06275809556245804], ['tiger', 'organism', 0.06294278055429459], ['stock', 'CD', 0.06611791253089905], ['smart', 'student', 0.06630216538906097], ['minority', 'peace', 0.06868557631969452], ['president', 'medal', 0.06946568936109543], ['stock', 'life', 0.07456468045711517], ['reason', 'hypertension', 0.07555150985717773], ['precedent', 'group', 0.07681484520435333], ['space', 'chemistry', 0.0802517756819725], ['announcement', 'production', 0.0806659460067749], ['cup', 'article', 0.08339213579893112], ['line', 'insurance', 0.08391070365905762], ['money', 'operation', 0.0845692902803421], ['planet', 'people', 0.08704743534326553], ['precedent', 'information', 0.08866603672504425], ['alcohol', 'chemistry', 0.08876198530197144], ['development', 'issue', 0.0901077538728714], ['energy', 'secretary', 0.09282554686069489], ['drink', 'ear', 0.09346937388181686], ['cup', 'food', 0.09379564225673676], ['direction', 'combination', 0.09585042297840118], ['word', 'similarity', 0.09658686816692352], ['practice', 'institution', 0.09677622467279434], ['peace', 'insurance', 0.09818778932094574], ['journey', 'car', 0.09849625825881958], ['report', 'gain', 0.09995907545089722], ['holy', 'sex', 0.10070592910051346], ['media', 'gain', 0.10171881318092346], ['problem', 'airport', 0.10208500921726227], ['computer', 'news', 0.1039980947971344], ['stock', 'egg', 0.10417767614126205], ['cup', 'object', 0.10605283081531525], ['listing', 'category', 0.11038407683372498], ['energy', 'laboratory', 0.11277153342962265], ['nature', 'man', 0.11616960912942886], ['prejudice', 'recognition', 0.11657065153121948], ['credit', 'information', 0.11662982404232025], ['shore', 'woodland', 0.116909459233284], ['king', 'cabbage', 0.11813169717788696], ['food', 'rooster', 0.11830645799636841], ['governor', 'interview', 0.11880449205636978], ['peace', 'plan', 0.11900727450847626], ['registration', 'arrangement', 0.11914842575788498], ['OPEC', 'country', 0.1194894015789032], ['psychology', 'clinic', 0.12102102488279343], ['summer', 'nature', 0.12260761111974716], ['stock', 'phone', 0.12326754629611969], ['listing', 'proximity', 0.1236044391989708], ['cup', 'substance', 0.12556418776512146], ['delay', 'news', 0.12586817145347595], ['month', 'hotel', 0.12591953575611115], ['media', 'trading', 0.12612037360668182], ['grocery', 'money', 0.12704519927501678], ['travel', 'activity', 0.12772586941719055], ['image', 'surface', 0.12843585014343262], ['shower', 'flood', 0.1294793337583542], ['space', 'world', 0.1296270787715912], ['theater', 'history', 0.12982811033725739], ['jaguar', 'car', 0.13319405913352966], ['peace', 'atmosphere', 0.13366280496120453], ['size', 'prominence', 0.1337670385837555], ['observation', 'architecture', 0.13629396259784698], ['Mars', 'water', 0.13637156784534454], ['profit', 'warning', 0.13709336519241333], ['phone', 'equipment', 0.13774040341377258], ['energy', 'crisis', 0.13847880065441132], ['territory', 'surface', 0.1395847499370575], ['focus', 'life', 0.14205680787563324], ['school', 'center', 0.14268213510513306], ['architecture', 'century', 0.1430933177471161], ['car', 'flight', 0.14423443377017975], ['disaster', 'area', 0.1452282965183258], ['hospital', 'infrastructure', 0.14553327858448029], ['chord', 'smile', 0.14869235455989838], ['dividend', 'calculation', 0.15065297484397888], ['man', 'governor', 0.15232418477535248], ['street', 'children', 0.15370695292949677], ['territory', 'kilometer', 0.1540842205286026], ['death', 'row', 0.1561247855424881], ['drink', 'mother', 0.15679940581321716], ['arrival', 'hotel', 0.1593550145626068], ['money', 'currency', 0.16010122001171112], ['doctor', 'personnel', 0.16101650893688202], ['coast', 'hill', 0.1611577719449997], ['journal', 'association', 0.16295820474624634], ['summer', 'drought', 0.1637890636920929], ['experience', 'music', 0.16575714945793152], ['psychology', 'fear', 0.17129430174827576], ['minister', 'party', 0.17155185341835022], ['opera', 'performance', 0.1716746687889099], ['drink', 'mouth', 0.1730400025844574], ['video', 'archive', 0.1735074818134308], ['movie', 'critic', 0.17430329322814941], ['population', 'development', 0.17502301931381226], ['victim', 'emergency', 0.17580637335777283], ['psychology', 'health', 0.17619232833385468], ['competition', 'price', 0.17668063938617706], ['street', 'place', 0.17753028869628906], ['boxing', 'round', 0.1775924563407898], ['food', 'preparation', 0.17799314856529236], ['production', 'crew', 0.1793084442615509], ['Mars', 'scientist', 0.17972101271152496], ['hundred', 'percent', 0.18021869659423828], ['situation', 'conclusion', 0.1847420334815979], ['ministry', 'culture', 0.187795490026474], ['start', 'year', 0.190186008810997], ['psychology', 'doctor', 0.19066518545150757], ['consumer', 'confidence', 0.19126541912555695], ['volunteer', 'motto', 0.19127660989761353], ['monk', 'slave', 0.19146226346492767], ['family', 'planning', 0.1914883255958557], ['life', 'lesson', 0.1925291270017624], ['planet', 'star', 0.19317106902599335], ['cup', 'tableware', 0.1948634535074234], ['soap', 'opera', 0.1955799013376236], ['lawyer', 'evidence', 0.1986079216003418], ['impartiality', 'interest', 0.20375370979309082], ['morality', 'importance', 0.2053404152393341], ['equipment', 'maker', 0.20798198878765106], ['king', 'rook', 0.20875594019889832], ['situation', 'isolation', 0.20984451472759247], ['country', 'citizen', 0.20985934138298035], ['tool', 'implement', 0.2123422920703888], ['professor', 'doctor', 0.21336083114147186], ['oil', 'stock', 0.2203981876373291], ['record', 'number', 0.22208692133426666], ['brother', 'monk', 0.22320020198822021], ['fuck', 'sex', 0.22339175641536713], ['life', 'term', 0.2267606556415558], ['forest', 'graveyard', 0.22901120781898499], ['arrangement', 'accommodation', 0.2305612713098526], ['announcement', 'effort', 0.23130609095096588], ['secretary', 'senate', 0.23210789263248444], ['lover', 'quarrel', 0.23226581513881683], ['viewer', 'serial', 0.23336467146873474], ['investigation', 'effort', 0.23487988114356995], ['stroke', 'hospital', 0.23570410907268524], ['coast', 'forest', 0.23609790205955505], ['chance', 'credibility', 0.23921890556812286], ['drug', 'abuse', 0.24085770547389984], ['canyon', 'landscape', 0.24369587004184723], ['luxury', 'car', 0.2445116937160492], ['drink', 'car', 0.24563539028167725], ['consumer', 'energy', 0.2461671233177185], ['announcement', 'warning', 0.24619008600711823], ['Arafat', 'Jackson', 0.24712520837783813], ['start', 'match', 0.24805231392383575], ['treatment', 'recovery', 0.2493065744638443], ['psychology', 'mind', 0.25049889087677], ['change', 'attitude', 0.2514270544052124], ['cup', 'liquid', 0.25183942914009094], ['money', 'laundering', 0.2523637115955353], ['asylum', 'madhouse', 0.2525393068790436], ['dollar', 'profit', 0.25337398052215576], ['dollar', 'buck', 0.2562117576599121], ['fertility', 'egg', 0.2565234899520874], ['weapon', 'secret', 0.2584390640258789], ['psychology', 'discipline', 0.2597163915634155], ['Wednesday', 'news', 0.2601482570171356], ['money', 'deposit', 0.2604113817214966], ['bank', 'money', 0.2613206207752228], ['money', 'bank', 0.2613206207752228], ['baseball', 'season', 0.2622438371181488], ['love', 'sex', 0.26393771171569824], ['movie', 'star', 0.26419907808303833], ['street', 'block', 0.26695922017097473], ['credit', 'card', 0.2681569457054138], ['money', 'property', 0.26824459433555603], ['atmosphere', 'landscape', 0.2697125971317291], ['psychology', 'anxiety', 0.2719925045967102], ['game', 'round', 0.2722221314907074], ['disability', 'death', 0.27521100640296936], ['death', 'inmate', 0.2757277488708496], ['deployment', 'departure', 0.2766171097755432], ['dividend', 'payment', 0.2785915434360504], ['planet', 'space', 0.28036749362945557], ['precedent', 'antecedent', 0.28377142548561096], ['concert', 'virtuoso', 0.28570181131362915], ['cell', 'phone', 0.28629887104034424], ['computer', 'laboratory', 0.2875465750694275], ['Maradona', 'football', 0.28883832693099976], ['game', 'series', 0.29130780696868896], ['century', 'nation', 0.291914701461792], ['network', 'hardware', 0.29377809166908264], ['government', 'crisis', 0.29561758041381836], ['governor', 'office', 0.29663556814193726], ['exhibit', 'memorabilia', 0.3025929927825928], ['hotel', 'reservation', 0.3028254508972168], ['bird', 'crane', 0.30286192893981934], ['monk', 'oracle', 0.3035403788089752], ['lobster', 'food', 0.30510464310646057], ['morality', 'marriage', 0.30624136328697205], ['planet', 'astronomer', 0.30787554383277893], ['news', 'report', 0.3123636543750763], ['psychology', 'depression', 0.3135565221309662], ['money', 'dollar', 0.3135656714439392], ['Arafat', 'peace', 0.3149864077568054], ['seafood', 'sea', 0.3150906562805176], ['lobster', 'wine', 0.3194861114025116], ['planet', 'sun', 0.3206743597984314], ['board', 'recommendation', 0.3211810886859894], ['Arafat', 'terror', 0.32154810428619385], ['decoration', 'valor', 0.3227156698703766], ['book', 'library', 0.324531227350235], ['money', 'wealth', 0.3295103907585144], ['tiger', 'fauna', 0.3297567069530487], ['lad', 'wizard', 0.33023008704185486], ['seven', 'series', 0.3310061991214752], ['day', 'dawn', 0.33108577132225037], ['game', 'victory', 0.3314370810985565], ['telephone', 'communication', 0.3321845233440399], ['museum', 'theater', 0.33256372809410095], ['game', 'defeat', 0.3329892158508301], ['psychology', 'Freud', 0.3330824673175812], ['century', 'year', 0.3348371386528015], ['FBI', 'fingerprint', 0.3378998339176178], ['currency', 'market', 0.3382996618747711], ['skin', 'eye', 0.3397478461265564], ['train', 'car', 0.34025612473487854], ['company', 'stock', 0.3415686786174774], ['profit', 'loss', 0.34199458360671997], ['shower', 'thunderstorm', 0.3426947295665741], ['fighting', 'defeating', 0.34452024102211], ['law', 'lawyer', 0.34465768933296204], ['television', 'film', 0.34557002782821655], ['precedent', 'law', 0.34597018361091614], ['cup', 'drink', 0.35322171449661255], ['cup', 'coffee', 0.3560177981853485], ['lad', 'brother', 0.359592467546463], ['life', 'death', 0.3618777096271515], ['weather', 'forecast', 0.3627207279205322], ['bird', 'cock', 0.3629024028778076], ['book', 'paper', 0.3634625971317291], ['liquid', 'water', 0.3653528094291687], ['physics', 'proton', 0.3664059638977051], ['glass', 'metal', 0.37073972821235657], ['precedent', 'example', 0.3727375268936157], ['food', 'fruit', 0.37409260869026184], ['plane', 'car', 0.3779698312282562], ['cemetery', 'woodland', 0.3819250762462616], ['street', 'avenue', 0.38327035307884216], ['media', 'radio', 0.3899160921573639], ['tennis', 'racket', 0.39200806617736816], ['reason', 'criterion', 0.3923366367816925], ['planet', 'constellation', 0.39312952756881714], ['water', 'seepage', 0.39313435554504395], ['computer', 'keyboard', 0.3963916301727295], ['psychology', 'cognition', 0.39951157569885254], ['nature', 'environment', 0.40284886956214905], ['five', 'month', 0.4040083885192871], ['jaguar', 'cat', 0.4054866135120392], ['deployment', 'withdrawal', 0.40605053305625916], ['computer', 'internet', 0.4068623483181], ['Japanese', 'American', 0.41248849034309387], ['FBI', 'investigation', 0.4171352982521057], ['wood', 'forest', 0.4175162613391876], ['closet', 'clothes', 0.4177303910255432], ['student', 'professor', 0.4206617772579193], ['tiger', 'feline', 0.42671453952789307], ['bishop', 'rabbi', 0.42778125405311584], ['tiger', 'carnivore', 0.42893749475479126], ['environment', 'ecology', 0.42997631430625916], ['bed', 'closet', 0.43194779753685], ['admission', 'ticket', 0.4368831515312195], ['physics', 'chemistry', 0.43719834089279175], ['movie', 'popcorn', 0.44535526633262634], ['day', 'summer', 0.4481317400932312], ['liability', 'insurance', 0.4524446129798889], ['rock', 'jazz', 0.45277100801467896], ['problem', 'challenge', 0.4629538059234619], ['marathon', 'sprint', 0.4636158347129822], ['announcement', 'news', 0.4673331677913666], ['stock', 'market', 0.4680556058883667], ['tiger', 'zoo', 0.4692455232143402], ['smart', 'stupid', 0.47047197818756104], ['movie', 'theater', 0.4741904139518738], ['war', 'troops', 0.47614771127700806], ['psychology', 'science', 0.4808834493160248], ['magician', 'wizard', 0.48634961247444153], ['benchmark', 'index', 0.49516886472702026], ['Mexico', 'Brazil', 0.49549198150634766], ['tiger', 'mammal', 0.49644753336906433], ['game', 'team', 0.4966561794281006], ['planet', 'moon', 0.5024792551994324], ['football', 'tennis', 0.5051179528236389], ['gender', 'equality', 0.5059923529624939], ['drink', 'eat', 0.5070200562477112], ['coast', 'shore', 0.5083667039871216], ['tiger', 'animal', 0.5096533298492432], ['tiger', 'cat', 0.5172962546348572], ['seafood', 'food', 0.5295588374137878], ['OPEC', 'oil', 0.5333777666091919], ['computer', 'software', 0.5444109439849854], ['midday', 'noon', 0.5527406334877014], ['tiger', 'jaguar', 0.5528685450553894], ['psychology', 'psychiatry', 0.5556287169456482], ['cucumber', 'potato', 0.5678562521934509], ['dollar', 'yen', 0.5745355486869812], ['car', 'automobile', 0.5838366746902466], ['baby', 'mother', 0.5839769840240479], ['boy', 'lad', 0.5886159539222717], ['vodka', 'gin', 0.5971748232841492], ['murder', 'manslaughter', 0.6057651042938232], ['calculation', 'computation', 0.6076955795288086], ['furnace', 'stove', 0.6083911061286926], ['television', 'radio', 0.6114970445632935], ['money', 'cash', 0.6151220798492432], ['money', 'cash', 0.6151220798492432], ['gem', 'jewel', 0.6210810542106628], ['doctor', 'nurse', 0.6319523453712463], ['planet', 'galaxy', 0.6338511109352112], ['Jerusalem', 'Palestinian', 0.6342512369155884], ['aluminum', 'metal', 0.6375763416290283], ['bread', 'butter', 0.641726016998291], ['king', 'queen', 0.6510956883430481], ['seafood', 'lobster', 0.6544079780578613], ['Jerusalem', 'Israel', 0.6638748645782471], ['championship', 'tournament', 0.6655317544937134], ['type', 'kind', 0.6666412949562073], ['football', 'basketball', 0.6682467460632324], ['journey', 'voyage', 0.6830853223800659], ['vodka', 'brandy', 0.6881492137908936], ['mile', 'kilometer', 0.7258477807044983], ['football', 'soccer', 0.731354832649231], ['man', 'woman', 0.7664011716842651], ['Harvard', 'Yale', 0.7817696332931519], ['tiger', 'tiger', 1.0000001192092896]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_scores(data):\n",
    "    scores = []\n",
    "\n",
    "    for w1, w2, mean in data:\n",
    "        # empty words\n",
    "        if not w1 or not w2:\n",
    "            continue\n",
    "\n",
    "        # word2vec as tensors for cos_score\n",
    "        w1_vec = torch.from_numpy(wv[w1])\n",
    "        w2_vec = torch.from_numpy(wv[w2])\n",
    "        cos_score = similarity(w1_vec, w2_vec)\n",
    "        \n",
    "        scores.append([w1, w2, cos_score.item()])\n",
    "    return sorted(scores, key=lambda entry: entry[2])\n",
    "    \n",
    "sim_scores = compute_similarity_scores(load_data())\n",
    "print(sim_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word pairs in the list based on the human judgement scores. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_judgement_scores(data):\n",
    "    return sorted(data, key=lambda entry: entry[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute spearman rank correlation between the two ranked lists obtained in the previous two steps.\n",
    "You do not need to implement the spearman rank correlation calculation from scratch. Feel free to use any Python library. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 'sex', 6.77), ('tiger', 'cat', 7.35), ('tiger', 'tiger', 10.0), ('book', 'paper', 7.46), ('computer', 'keyboard', 7.62), ('computer', 'internet', 7.58), ('plane', 'car', 5.77), ('train', 'car', 6.31), ('telephone', 'communication', 7.5), ('television', 'radio', 6.77), ('media', 'radio', 7.42), ('drug', 'abuse', 6.85), ('bread', 'butter', 6.19), ('cucumber', 'potato', 5.92), ('doctor', 'nurse', 7.0), ('professor', 'doctor', 6.62), ('student', 'professor', 6.81), ('smart', 'student', 4.62), ('smart', 'stupid', 5.81), ('company', 'stock', 7.08), ('stock', 'market', 8.08), ('stock', 'phone', 1.62), ('stock', 'CD', 1.31), ('stock', 'jaguar', 0.92), ('stock', 'egg', 1.81), ('fertility', 'egg', 6.69), ('stock', 'live', 3.73), ('stock', 'life', 0.92), ('book', 'library', 7.46), ('bank', 'money', 8.12), ('wood', 'forest', 7.73), ('money', 'cash', 9.15), ('professor', 'cucumber', 0.31), ('king', 'cabbage', 0.23), ('king', 'queen', 8.58), ('king', 'rook', 5.92), ('bishop', 'rabbi', 6.69), ('Jerusalem', 'Israel', 8.46), ('Jerusalem', 'Palestinian', 7.65), ('holy', 'sex', 1.62), ('fuck', 'sex', 9.44), ('Maradona', 'football', 8.62), ('football', 'soccer', 9.03), ('football', 'basketball', 6.81), ('football', 'tennis', 6.63), ('tennis', 'racket', 7.56), ('Arafat', 'peace', 6.73), ('Arafat', 'terror', 7.65), ('Arafat', 'Jackson', 2.5), ('law', 'lawyer', 8.38), ('movie', 'star', 7.38), ('movie', 'popcorn', 6.19), ('movie', 'critic', 6.73), ('movie', 'theater', 7.92), ('physics', 'proton', 8.12), ('physics', 'chemistry', 7.35), ('space', 'chemistry', 4.88), ('alcohol', 'chemistry', 5.54), ('vodka', 'gin', 8.46), ('vodka', 'brandy', 8.13), ('drink', 'car', 3.04), ('drink', 'ear', 1.31), ('drink', 'mouth', 5.96), ('drink', 'eat', 6.87), ('baby', 'mother', 7.85), ('drink', 'mother', 2.65), ('car', 'automobile', 8.94), ('gem', 'jewel', 8.96), ('journey', 'voyage', 9.29), ('boy', 'lad', 8.83), ('coast', 'shore', 9.1), ('asylum', 'madhouse', 8.87), ('magician', 'wizard', 9.02), ('midday', 'noon', 9.29), ('furnace', 'stove', 8.79), ('food', 'fruit', 7.52), ('bird', 'cock', 7.1), ('bird', 'crane', 7.38), ('tool', 'implement', 6.46), ('brother', 'monk', 6.27), ('crane', 'implement', 2.69), ('lad', 'brother', 4.46), ('journey', 'car', 5.85), ('monk', 'oracle', 5.0), ('cemetery', 'woodland', 2.08), ('food', 'rooster', 4.42), ('coast', 'hill', 4.38), ('forest', 'graveyard', 1.85), ('shore', 'woodland', 3.08), ('monk', 'slave', 0.92), ('coast', 'forest', 3.15), ('lad', 'wizard', 0.92), ('chord', 'smile', 0.54), ('glass', 'magician', 2.08), ('noon', 'string', 0.54), ('rooster', 'voyage', 0.62), ('money', 'dollar', 8.42), ('money', 'cash', 9.08), ('money', 'currency', 9.04), ('money', 'wealth', 8.27), ('money', 'property', 7.57), ('money', 'possession', 7.29), ('money', 'bank', 8.5), ('money', 'deposit', 7.73), ('money', 'withdrawal', 6.88), ('money', 'laundering', 5.65), ('money', 'operation', 3.31), ('tiger', 'jaguar', 8.0), ('tiger', 'feline', 8.0), ('tiger', 'carnivore', 7.08), ('tiger', 'mammal', 6.85), ('tiger', 'animal', 7.0), ('tiger', 'organism', 4.77), ('tiger', 'fauna', 5.62), ('tiger', 'zoo', 5.87), ('psychology', 'psychiatry', 8.08), ('psychology', 'anxiety', 7.0), ('psychology', 'fear', 6.85), ('psychology', 'depression', 7.42), ('psychology', 'clinic', 6.58), ('psychology', 'doctor', 6.42), ('psychology', 'Freud', 8.21), ('psychology', 'mind', 7.69), ('psychology', 'health', 7.23), ('psychology', 'science', 6.71), ('psychology', 'discipline', 5.58), ('psychology', 'cognition', 7.48), ('planet', 'star', 8.45), ('planet', 'constellation', 8.06), ('planet', 'moon', 8.08), ('planet', 'sun', 8.02), ('planet', 'galaxy', 8.11), ('planet', 'space', 7.92), ('planet', 'astronomer', 7.94), ('precedent', 'example', 5.85), ('precedent', 'information', 3.85), ('precedent', 'cognition', 2.81), ('precedent', 'law', 6.65), ('precedent', 'collection', 2.5), ('precedent', 'group', 1.77), ('precedent', 'antecedent', 6.04), ('cup', 'coffee', 6.58), ('cup', 'tableware', 6.85), ('cup', 'article', 2.4), ('cup', 'artifact', 2.92), ('cup', 'object', 3.69), ('cup', 'entity', 2.15), ('cup', 'drink', 7.25), ('cup', 'food', 5.0), ('cup', 'substance', 1.92), ('cup', 'liquid', 5.9), ('jaguar', 'cat', 7.42), ('jaguar', 'car', 7.27), ('energy', 'secretary', 1.81), ('secretary', 'senate', 5.06), ('energy', 'laboratory', 5.09), ('computer', 'laboratory', 6.78), ('weapon', 'secret', 6.06), ('FBI', 'fingerprint', 6.94), ('FBI', 'investigation', 8.31), ('investigation', 'effort', 4.59), ('Mars', 'water', 2.94), ('Mars', 'scientist', 5.63), ('news', 'report', 8.16), ('canyon', 'landscape', 7.53), ('image', 'surface', 4.56), ('discovery', 'space', 6.34), ('water', 'seepage', 6.56), ('sign', 'recess', 2.38), ('Wednesday', 'news', 2.22), ('mile', 'kilometer', 8.66), ('computer', 'news', 4.47), ('territory', 'surface', 5.34), ('atmosphere', 'landscape', 3.69), ('president', 'medal', 3.0), ('war', 'troops', 8.13), ('record', 'number', 6.31), ('skin', 'eye', 6.22), ('Japanese', 'American', 6.5), ('theater', 'history', 3.91), ('volunteer', 'motto', 2.56), ('prejudice', 'recognition', 3.0), ('decoration', 'valor', 5.63), ('century', 'year', 7.59), ('century', 'nation', 3.16), ('delay', 'racism', 1.19), ('delay', 'news', 3.31), ('minister', 'party', 6.63), ('peace', 'plan', 4.75), ('minority', 'peace', 3.69), ('attempt', 'peace', 4.25), ('government', 'crisis', 6.56), ('deployment', 'departure', 4.25), ('deployment', 'withdrawal', 5.88), ('energy', 'crisis', 5.94), ('announcement', 'news', 7.56), ('announcement', 'effort', 2.75), ('stroke', 'hospital', 7.03), ('disability', 'death', 5.47), ('victim', 'emergency', 6.47), ('treatment', 'recovery', 7.91), ('journal', 'association', 4.97), ('doctor', 'personnel', 5.0), ('doctor', 'liability', 5.19), ('liability', 'insurance', 7.03), ('school', 'center', 3.44), ('reason', 'hypertension', 2.31), ('reason', 'criterion', 5.91), ('hundred', 'percent', 7.38), ('Harvard', 'Yale', 8.13), ('hospital', 'infrastructure', 4.63), ('death', 'row', 5.25), ('death', 'inmate', 5.03), ('lawyer', 'evidence', 6.69), ('life', 'death', 7.88), ('life', 'term', 4.5), ('word', 'similarity', 4.75), ('board', 'recommendation', 4.47), ('governor', 'interview', 3.25), ('OPEC', 'country', 5.63), ('peace', 'atmosphere', 3.69), ('peace', 'insurance', 2.94), ('territory', 'kilometer', 5.28), ('travel', 'activity', 5.0), ('competition', 'price', 6.44), ('consumer', 'confidence', 4.13), ('consumer', 'energy', 4.75), ('problem', 'airport', 2.38), ('car', 'flight', 4.94), ('credit', 'card', 8.06), ('credit', 'information', 5.31), ('hotel', 'reservation', 8.03), ('grocery', 'money', 5.94), ('registration', 'arrangement', 6.0), ('arrangement', 'accommodation', 5.41), ('month', 'hotel', 1.81), ('type', 'kind', 8.97), ('arrival', 'hotel', 6.0), ('bed', 'closet', 6.72), ('closet', 'clothes', 8.0), ('situation', 'conclusion', 4.81), ('situation', 'isolation', 3.88), ('impartiality', 'interest', 5.16), ('direction', 'combination', 2.25), ('street', 'place', 6.44), ('street', 'avenue', 8.88), ('street', 'block', 6.88), ('street', 'children', 4.94), ('listing', 'proximity', 2.56), ('listing', 'category', 6.38), ('cell', 'phone', 7.81), ('production', 'hike', 1.75), ('benchmark', 'index', 4.25), ('media', 'trading', 3.88), ('media', 'gain', 2.88), ('dividend', 'payment', 7.63), ('dividend', 'calculation', 6.48), ('calculation', 'computation', 8.44), ('currency', 'market', 7.5), ('OPEC', 'oil', 8.59), ('oil', 'stock', 6.34), ('announcement', 'production', 3.38), ('announcement', 'warning', 6.0), ('profit', 'warning', 3.88), ('profit', 'loss', 7.63), ('dollar', 'yen', 7.78), ('dollar', 'buck', 9.22), ('dollar', 'profit', 7.38), ('dollar', 'loss', 6.09), ('computer', 'software', 8.5), ('network', 'hardware', 8.31), ('phone', 'equipment', 7.13), ('equipment', 'maker', 5.91), ('luxury', 'car', 6.47), ('five', 'month', 3.38), ('report', 'gain', 3.63), ('investor', 'earning', 7.13), ('liquid', 'water', 7.89), ('baseball', 'season', 5.97), ('game', 'victory', 7.03), ('game', 'team', 7.69), ('marathon', 'sprint', 7.47), ('game', 'series', 6.19), ('game', 'defeat', 6.97), ('seven', 'series', 3.56), ('seafood', 'sea', 7.47), ('seafood', 'food', 8.34), ('seafood', 'lobster', 8.7), ('lobster', 'food', 7.81), ('lobster', 'wine', 5.7), ('food', 'preparation', 6.22), ('video', 'archive', 6.34), ('start', 'year', 4.06), ('start', 'match', 4.47), ('game', 'round', 5.97), ('boxing', 'round', 7.61), ('championship', 'tournament', 8.36), ('fighting', 'defeating', 7.41), ('line', 'insurance', 2.69), ('day', 'summer', 3.94), ('summer', 'drought', 7.16), ('summer', 'nature', 5.63), ('day', 'dawn', 7.53), ('nature', 'environment', 8.31), ('environment', 'ecology', 8.81), ('nature', 'man', 6.25), ('man', 'woman', 8.3), ('man', 'governor', 5.25), ('murder', 'manslaughter', 8.53), ('soap', 'opera', 7.94), ('opera', 'performance', 6.88), ('life', 'lesson', 5.94), ('focus', 'life', 4.06), ('production', 'crew', 6.25), ('television', 'film', 7.72), ('lover', 'quarrel', 6.19), ('viewer', 'serial', 2.97), ('possibility', 'girl', 1.94), ('population', 'development', 3.75), ('morality', 'importance', 3.31), ('morality', 'marriage', 3.69), ('Mexico', 'Brazil', 7.44), ('gender', 'equality', 6.41), ('change', 'attitude', 5.44), ('family', 'planning', 6.25), ('opera', 'industry', 2.63), ('sugar', 'approach', 0.88), ('practice', 'institution', 3.19), ('ministry', 'culture', 4.69), ('problem', 'challenge', 6.75), ('size', 'prominence', 5.31), ('country', 'citizen', 7.31), ('planet', 'people', 5.75), ('development', 'issue', 3.97), ('experience', 'music', 3.47), ('music', 'project', 3.63), ('glass', 'metal', 5.56), ('aluminum', 'metal', 7.83), ('chance', 'credibility', 3.88), ('exhibit', 'memorabilia', 5.31), ('concert', 'virtuoso', 6.81), ('rock', 'jazz', 7.59), ('museum', 'theater', 7.19), ('observation', 'architecture', 4.38), ('space', 'world', 6.53), ('preservation', 'world', 6.19), ('admission', 'ticket', 7.69), ('shower', 'thunderstorm', 6.31), ('shower', 'flood', 6.03), ('weather', 'forecast', 8.34), ('disaster', 'area', 6.25), ('governor', 'office', 6.34), ('architecture', 'century', 3.78)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_spearman():\n",
    "    data = load_data()\n",
    "    print(data)\n",
    "\n",
    "    sim_score = np.array(compute_similarity_scores(data))\n",
    "    human_score = np.array(human_judgement_scores(data))\n",
    "\n",
    "    # use third column of dataset (mean/cos_similarity)\n",
    "    preds = torch.from_numpy(sim_score[:, 2].astype(float))\n",
    "    target = torch.from_numpy(human_score[:, 2].astype(float))\n",
    "\n",
    "    spearman = SpearmanCorrCoef()\n",
    "    return spearman(preds, target)\n",
    "    \n",
    "compute_spearman()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding based clasifier\n",
    "We will design a simple sentiment classifier based on the pre-trained word embeddings (google news).\n",
    "\n",
    "Each data point is a movie review and the sentiment could be either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('sentiment_test_X.p', 'rb') as fs:\n",
    "    test_X = pickle.load(fs)\n",
    "\n",
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'sometimes',\n",
       " 'like',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'movies',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fun',\n",
       " ',',\n",
       " 'Wasabi',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'start',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment_test_y.p', 'rb') as fs:\n",
    "    test_y = pickle.load(fs)\n",
    "    \n",
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_train_X.p', 'rb') as fs:\n",
    "    train_X = pickle.load(fs)\n",
    "with open('sentiment_train_y.p', 'rb') as fs:\n",
    "    train_y = pickle.load(fs)\n",
    "with open('sentiment_val_X.p', 'rb') as fs:\n",
    "    val_X = pickle.load(fs)\n",
    "with open('sentiment_val_y.p', 'rb') as fs:\n",
    "    val_y = pickle.load(fs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, compute its embedding by averaging over the embedding of its constituent words. Define a function which given a review as a list of words, generates its embeddings by averaging over the constituent word embeddings. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(review):\n",
    "    res = np.mean([wv[word] for word in review if word in wv], axis=0)\n",
    "    if np.isnan(res).any():\n",
    "        return np.zeros(300)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feed-forward network class with pytorch. (Hyperparamter choice such as number of layers, hidden size is left to you) (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.fc_1 = nn.Linear(self.input_size, self.hidden_1_size)\n",
    "        self.fc_2 = nn.Linear(self.hidden_1_size, self.hidden_2_size)\n",
    "        self.fc_3 = nn.Linear(self.hidden_2_size, self.output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.fc_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_1 = self.relu(self.fc_1(x))\n",
    "        h_2 = self.relu(self.fc_2(h_1))\n",
    "        y = self.fc_3(h_2)\n",
    "        return self.sigmoid(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset class for efficiently enumerating over the dataset. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sent_data(Dataset):\n",
    "    def __init__(self, data_points, class_labels):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data_points\n",
    "        self.labels = class_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        d = torch.FloatTensor(generate_embedding(self.data[index]))\n",
    "        l = torch.FloatTensor([self.labels[index]])\n",
    "        return d,l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train model. At the end of each epoch compute the validation accuracy and save the model with the best validation accuracy. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to calculate the accuracy\n",
    "def calc_accuracy(gts, preds):\n",
    "    correct = sum(1 for tc, pred in zip(gts, preds) if tc == pred)\n",
    "    total = len(gts)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "# Adopt your code to incorporate mini-batch training\n",
    "# Use cross-entropy as your loss function\n",
    "def train(model, train_data, val_data, batch_size, epochs=5, learning_rate=0.001):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    lossfn = nn.BCELoss()\n",
    "    best_accuracy = 0.0\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        for d, l in train_dataloader:\n",
    "            out = model(d)\n",
    "            loss =  lossfn(out, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        true_classes = []\n",
    "        predicted_classes = []\n",
    "        for d, l in val_dataloader:\n",
    "            out = model(d)\n",
    "            true_classes.extend(l.squeeze(1).tolist())\n",
    "            predictions = torch.where(out > 0.5, 1, 0) # for two classes and out in (0,1) this should be enough for class probabilities? \n",
    "            predicted_classes.extend(predictions.squeeze(1).tolist())\n",
    "            \n",
    "        # Calculate the Accuracy\n",
    "        accuracy = calc_accuracy(true_classes, predicted_classes)\n",
    "\n",
    "        #Check whether current model is better than best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model.state_dict()\n",
    "            \n",
    "    # Save the best model\n",
    "    torch.save(best_model, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set and report the test accuracy. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    model.eval()\n",
    "    test_dataloader = DataLoader(test_data)\n",
    "    true_classes = []\n",
    "    predicted_classes = []\n",
    "    for d,l in test_dataloader:\n",
    "        out = model(d)\n",
    "        true_classes.extend(l)\n",
    "        predictions = torch.where(out > 0.5, 1, 0)\n",
    "        predicted_classes.extend(predictions.squeeze(1).tolist())\n",
    "        \n",
    "    # Calculate the Accuracy\n",
    "    accuracy = calc_accuracy(true_classes, predicted_classes)\n",
    "\n",
    "    print(\"The accuracy of the best model on the test set is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30/30 [00:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the best model on the test set is: 0.7808896210873146\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER: Maybe implement GPU training for faster execution\n",
    "train_data = sent_data(train_X, train_y)\n",
    "val_data = sent_data(val_X, val_y)\n",
    "test_data = sent_data(test_X, test_y)\n",
    "model = Classifier(300, 100, 50, 1)\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "train(model, train_data, val_data, batch_size, epochs)\n",
    "best_model_state = torch.load('best_model.pth')\n",
    "model.load_state_dict(best_model_state)\n",
    "evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([4.3299e-13], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "print(model.forward(torch.Tensor(wv[\"good\"])))\n",
    "print(model.forward(torch.Tensor(wv[\"awful\"])))\n",
    "print(model.forward(torch.Tensor(wv[\"eventually\"])))\n",
    "print(model.forward(torch.Tensor(wv[\"boring\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
